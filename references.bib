@article{amodei2016concrete,
  title={Concrete problems in AI safety},
  author={Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  journal={arXiv preprint arXiv:1606.06565},
  year={2016}
}

@article{armstrong2012thinking,
  title={Thinking inside the box: Controlling and using an oracle AI},
  author={Armstrong, Stuart and Sandberg, Anders and Bostrom, Nick},
  journal={Minds and Machines},
  volume={22},
  number={4},
  pages={299--324},
  year={2012},
  publisher={Springer}
}

@article{bai2022constitutional,
  title={Constitutional AI: Harmlessness from AI feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@book{bostrom2014superintelligence,
  title={Superintelligence: Paths, dangers, strategies},
  author={Bostrom, Nick},
  year={2014},
  publisher={Oxford University Press}
}

@article{carlsmith2022power,
  title={Is power-seeking AI an existential risk?},
  author={Carlsmith, Joseph},
  journal={arXiv preprint arXiv:2206.13353},
  year={2022}
}

@inproceedings{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  booktitle={Advances in neural information processing systems},
  pages={4299--4307},
  year={2017}
}

@misc{cotra2021alignment,
  title={Why AI alignment could be hard with modern deep learning},
  author={Cotra, Ajeya},
  journal={Cold Takes},
  year={2021}
}

@article{demski2019embedded,
  title={Embedded agency},
  author={Demski, Abram and Garrabrant, Scott},
  journal={arXiv preprint arXiv:1902.09469},
  year={2019}
}

@article{gabriel2020artificial,
  title={Artificial intelligence, values, and alignment},
  author={Gabriel, Iason},
  journal={Minds and machines},
  volume={30},
  number={3},
  pages={411--437},
  year={2020},
  publisher={Springer}
}

@article{hubinger2019risks,
  title={Risks from learned optimization in advanced machine learning systems},
  author={Hubinger, Evan and van Merwijk, Chris and Mikulik, Vladimir and Skalse, Joar and Garrabrant, Scott},
  journal={arXiv preprint arXiv:1906.01820},
  year={2019}
}

@article{irving2018debate,
  title={AI safety via debate},
  author={Irving, Geoffrey and Christiano, Paul and Amodei, Dario},
  journal={arXiv preprint arXiv:1805.00899},
  year={2018}
}

@article{kenton2021alignment,
  title={Alignment of language agents},
  author={Kenton, Zachary and Everitt, Tom and Weidinger, Laura and Gabriel, Iason and Mikulik, Vladimir and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2103.14659},
  year={2021}
}

@article{leike2018scalable,
  title={Scalable agent alignment via reward modeling: a research direction},
  author={Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
  journal={arXiv preprint arXiv:1811.07871},
  year={2018}
}

@article{ngo2022alignment,
  title={The alignment problem from a deep learning perspective},
  author={Ngo, Richard and Chan, Lawrence and Mindermann, S{\"o}ren},
  journal={arXiv preprint arXiv:2209.00626},
  year={2022}
}

@article{ogara2023distributional,
  title={Auto-induced distributional shift in reinforcement learning from human feedback},
  author={O'Gara, Aidan},
  journal={arXiv preprint arXiv:2310.08380},
  year={2023}
}

@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}

@inproceedings{omohundro2008basic,
  title={The basic AI drives},
  author={Omohundro, Stephen M},
  booktitle={Proceedings of the 2008 conference on Artificial General Intelligence},
  pages={483--492},
  year={2008}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}

@article{perez2022red,
  title={Red teaming language models with language models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and others},
  journal={arXiv preprint arXiv:2202.03286},
  year={2022}
}

@book{russell2019human,
  title={Human compatible: Artificial intelligence and the problem of control},
  author={Russell, Stuart},
  year={2019},
  publisher={Viking}
}

@article{shah2022goal,
  title={Goal misgeneralization: Why correct specifications aren't enough for correct goals},
  author={Shah, Rohin and Varma, Vikrant and Kumar, Ramana and Phuong, Mary and Krakovna, Victoria and Uesato, Jonathan and Kenton, Zachary},
  journal={arXiv preprint arXiv:2210.01790},
  year={2022}
}

@article{sharma2023sycophancy,
  title={Towards understanding sycophancy in language models},
  author={Sharma, Mrinank and Tong, Meg and Korbak, Tomasz and Duvenaud, David and Askell, Amanda and Bowman, Samuel R and others},
  journal={arXiv preprint arXiv:2310.13548},
  year={2023}
}

@inproceedings{soares2015corrigibility,
  title={Corrigibility},
  author={Soares, Nate and Fallenstein, Benja and Armstrong, Stuart and Yudkowsky, Eliezer},
  booktitle={Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence},
  year={2015}
}

@article{steinhardt2022more,
  title={More is different for AI},
  author={Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2205.04305},
  year={2022}
}

@article{strathern1997improving,
  title={'Improving ratings': audit in the British University system},
  author={Strathern, Marilyn},
  journal={European review},
  volume={5},
  number={3},
  pages={305--321},
  year={1997},
  publisher={Cambridge University Press}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{turner2021optimal,
  title={Optimal policies tend to seek power},
  author={Turner, Alex M and Smith, Logan and Shah, Rohin and Critch, Andrew and Tadepalli, Prasad},
  journal={arXiv preprint arXiv:2105.14111},
  year={2021}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{wei2023jailbroken,
  title={Jailbroken: How does LLM safety training fail?},
  author={Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2307.02483},
  year={2023}
}

@article{weidinger2021ethical,
  title={Ethical and social risks of harm from language models},
  author={Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and others},
  journal={arXiv preprint arXiv:2112.04359},
  year={2021}
}

@incollection{yudkowsky2008artificial,
  title={Artificial intelligence as a positive and negative factor in global risk},
  author={Yudkowsky, Eliezer},
  booktitle={Global catastrophic risks},
  volume={1},
  number={303},
  pages={184},
  year={2008}
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and others},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

@techreport{anthropic2025sonnet,
  title={Claude Sonnet 4.5 System Card},
  author={{Anthropic}},
  year={2025},
  institution={Anthropic},
  url={https://assets.anthropic.com/m/12f214efcc2f457a/original/Claude-Sonnet-4-5-System-Card.pdf},
  note={Accessed: 2025-10-05}
}

@article{openai2025monitoring,
  title={Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation},
  author={{OpenAI}},
  journal={arXiv preprint arXiv:2503.11926},
  year={2025},
  url={https://arxiv.org/abs/2503.11926}
}

@article{greenblatt2024alignment,
  title={Alignment Faking in Large Language Models},
  author={Greenblatt, Ryan and Denison, Carson and Wright, Benjamin and Roger, Fabien and MacDiarmid, Monte and others},
  journal={arXiv preprint arXiv:2412.14093},
  year={2024},
  url={https://arxiv.org/abs/2412.14093}
}

@techreport{apollo2024scheming,
  title={Frontier Models are Capable of In-context Scheming},
  author={{Apollo Research}},
  year={2024},
  institution={Apollo Research},
  url={https://www.apolloresearch.ai/research/scheming-reasoning-evaluations}
}

@article{langosco2022goal,
  title={Goal Misgeneralization in Deep Reinforcement Learning},
  author={Langosco, Lauro and Koch, Jack and Sharkey, Lee D and Pfau, Jacob and Krueger, David},
  journal={arXiv preprint arXiv:2105.14111},
  year={2022}
}

@article{turner2021optimal,
  title={Optimal Policies Tend to Seek Power},
  author={Turner, Alexander Matt and Smith, Logan and Shah, Rohin and Critch, Andrew and Tadepalli, Prasad},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={23063--23074},
  year={2021}
}
